{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Project Delivery #3 - Deep Learning based Text Classification - Predict Pledged Money</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name              object\n",
      "main_category     object\n",
      "deadline          object\n",
      "goal             float64\n",
      "launched          object\n",
      "pledged          float64\n",
      "backers            int64\n",
      "country           object\n",
      "dtype: object\n",
      "name              object\n",
      "main_category       int8\n",
      "goal             float64\n",
      "country             int8\n",
      "dtype: object\n",
      "float64\n",
      "375858\n",
      "243441\n",
      "243441\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import texthero as hero\n",
    "from texthero import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "ks2018 = pd.read_csv(\"ks-projects-201801.csv\")\n",
    "#ks2016 = pd.read_csv(\"ks-projects-201612.csv\")\n",
    "#print(ks2016.info())\n",
    "\n",
    "df=ks2018\n",
    "# remove currently live campaigns\n",
    "df = df[df.state != 'live']\n",
    "\n",
    "# drop columns we don't need\n",
    "df = df.drop('ID', axis=1)\n",
    "df = df.drop('category', axis=1) #we keep main category\n",
    "df = df.drop('state', axis=1)\n",
    "df = df.drop('usd pledged', axis=1)\n",
    "df = df.drop(\"usd_pledged_real\", axis=1)\n",
    "df = df.drop(\"usd_goal_real\", axis=1)\n",
    "df = df.drop('currency', axis=1)\n",
    "\n",
    "\n",
    "df = df[df['name'].notna()]\n",
    "custom_pipeline = [preprocessing.fillna,\n",
    "                   #preprocessing.lowercase,\n",
    "                   preprocessing.remove_whitespace,\n",
    "                   preprocessing.remove_diacritics\n",
    "                   #preprocessing.remove_brackets\n",
    "                  ]\n",
    "df['name'] = hero.clean(df['name'], custom_pipeline)\n",
    "df['name'] = [n.replace('{','') for n in df['name']]\n",
    "df['name'] = [n.replace('}','') for n in df['name']]\n",
    "df['name'] = [n.replace('(','') for n in df['name']]\n",
    "df['name'] = [n.replace(')','') for n in df['name']]\n",
    "\n",
    "print(df.dtypes)\n",
    "\n",
    "X=df\n",
    "X=X.drop([ 'deadline', 'launched', 'backers','pledged'], axis=1)\n",
    "y=df[\"pledged\"]/df[\"goal\"]\n",
    "\n",
    "#encoding category and country //nominal attributes\n",
    "X[\"main_category\"] = X[\"main_category\"].astype('category')\n",
    "X[\"main_category\"] = X[\"main_category\"].cat.codes\n",
    "X[\"country\"] = X[\"country\"].astype('category')\n",
    "X[\"country\"] = X[\"country\"].cat.codes\n",
    "\n",
    "print(X.dtypes)\n",
    "print(y.dtypes)\n",
    "\n",
    "#Removing Outliers\n",
    "y1= y.between(0, 1, inclusive=True) \n",
    "y1= y1.index[y1 == True].tolist()\n",
    "print(len(y))\n",
    "#print(y1)\n",
    "y=y[y1]\n",
    "X=X.loc[y1,:]\n",
    "print(len(X))\n",
    "print(len(y))\n",
    "\n",
    "#https://github.com/sdevalapurkar/kickstarter-prediction\n",
    "#https://towardsdatascience.com/how-to-vectorize-text-in-dataframes-for-nlp-tasks-3-simple-techniques-82925a5600db\n",
    "#https://www.kaggle.com/stacykurnikova/using-glove-embedding\n",
    "#https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "#https://djajafer.medium.com/multi-class-text-classification-with-keras-and-lstm-4c5525bef592\n",
    "#https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "#https://realpython.com/python-keras-text-classification/#convolutional-neural-networks-cnn\n",
    "#https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe data loaded\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.300d.txt',encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0] ## The first entry is the word\n",
    "    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('GloVe data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "## Iterate over the data to preprocess by removing stopwords\n",
    "lines_without_stopwords=[] \n",
    "for line in X['name'].values: \n",
    "    line = line.lower()\n",
    "    line_by_words = re.findall(r'(?:\\w+)', line, flags = re.UNICODE) # remove punctuation ans split\n",
    "    new_line=[]\n",
    "    for word in line_by_words:\n",
    "        if word not in stop:\n",
    "            new_line.append(word)\n",
    "    lines_without_stopwords.append(new_line)\n",
    "texts = lines_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 94029 unique tokens.\n",
      "Found 37918 unique tokens.\n",
      "(194752, 100)\n",
      "(48689, 100)\n"
     ]
    }
   ],
   "source": [
    "## Code adapted from (https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py)\n",
    "# Vectorize the text samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X['name']=texts\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X.iloc[:,1:])\n",
    "normalized_arr=scaler.transform(X.iloc[:,1:])\n",
    "X.iloc[:,1:]=normalized_arr\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,shuffle=True)\n",
    "\n",
    "MAX_NUM_WORDS = 1000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "tokenizertrain = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizertrain.fit_on_texts(X_train[\"name\"])\n",
    "sequences = tokenizertrain.texts_to_sequences(X_train[\"name\"])\n",
    "\n",
    "word_index = tokenizertrain.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X_train_text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "tokenizertest = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizertest.fit_on_texts(X_test[\"name\"])\n",
    "sequences = tokenizertest.texts_to_sequences(X_test[\"name\"])\n",
    "\n",
    "word_index = tokenizertest.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X_test_text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_train_nottext=X_train.drop('name', axis=1)\n",
    "X_test_nottext=X_test.drop('name', axis=1)\n",
    "\n",
    "print(X_train_text.shape)\n",
    "print(X_test_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More code adapted from the keras reference (https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py)\n",
    "# prepare embedding matrix \n",
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, GlobalMaxPooling1D, Dropout, Activation, MaxPooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "## EMBEDDING_DIM =  ## seems to need to match the embeddings_index dimension\n",
    "EMBEDDING_DIM = embeddings_index.get('a').shape[0]\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word) ## This references the loaded embeddings dictionary\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim, regress=False): #Multi-layer perceptron for nominal, numeric data\n",
    "\t# define our MLP network\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "\tmodel.add(Dense(4, activation=\"relu\"))\n",
    "\t# check to see if the regression node should be added\n",
    "\tif regress:\n",
    "\t\tmodel.add(Dense(1, activation=\"linear\"))\n",
    "\t# return our model\n",
    "\treturn model\n",
    "\n",
    "mlpModel=create_mlp(X_train_nottext.shape[1], regress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          300300    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 96, 128)           192128    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 497,152\n",
      "Trainable params: 196,852\n",
      "Non-trainable params: 300,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelcnn = Sequential()\n",
    "modelcnn.add(Embedding(num_words, 300, input_length=100, weights= [embedding_matrix], trainable=False))\n",
    "modelcnn.add(Conv1D(128, 5, activation='relu'))\n",
    "modelcnn.add(GlobalMaxPooling1D())\n",
    "modelcnn.add(Dense(32, activation='relu'))\n",
    "modelcnn.add(Dense(16, activation='relu'))\n",
    "modelcnn.add(Dense(4, activation='relu'))\n",
    "modelcnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.constraints import nonneg\n",
    "\n",
    "combinedInput = concatenate([mlpModel.output, modelcnn.output])\n",
    "# our final FC layer head will have two dense layers, the final one\n",
    "# being our regression head\n",
    "x = Dense(4, activation=\"relu\")(combinedInput)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "# our final model will accept categorical/numerical data on the MLP\n",
    "# input and texts on the CNN input, outputting a single value //successful or not\n",
    "model = Model(inputs=[mlpModel.input, modelcnn.input], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "6086/6086 [==============================] - 113s 19ms/step - loss: 67703509.2663 - mse: 0.1153 - mae: 0.2868 - mape: 67703509.2663 - val_loss: 12869646.0000 - val_mse: 0.0546 - val_mae: 0.1337 - val_mape: 12869646.0000\n",
      "[INFO] predicting pledged money...\n",
      "38450\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=1e-5, decay=1e-5)\n",
    "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt, metrics=['mse', 'mae', 'mape'])\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "\n",
    "model.fit(\n",
    "\tx=[X_train_nottext, X_train_text], y=y_train,\n",
    "\tvalidation_data=([X_test_nottext, X_test_text], y_test),\n",
    "\tepochs=1)\n",
    "# make predictions on the testing data\n",
    "print(\"[INFO] predicting pledged money...\")\n",
    "preds = model.predict([X_test_nottext, X_test_text])\n",
    "axs=np.unique(preds)\n",
    "print(len(axs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted minimum: 0.0000000062358554\n",
      "predicted maximum: 0.3552994430065155\n",
      "predicted mean: 0.0559774860739708\n",
      "test value mean: 0.1037705110751163\n",
      "test value minimum: 0.0000000000000000\n",
      "test value maximum: 1.0000000000000000\n",
      "mse: 0.23356446957152685\n",
      "mae: 0.1336586111550642\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "print(\"predicted minimum: {:.16f}\".format(preds.min()))\n",
    "print(\"predicted maximum: {:.16f}\".format(preds.max()))\n",
    "print(\"predicted mean: {:.16f}\".format(preds.mean()))\n",
    "print(\"test value mean: {:.16f}\".format(y_test.mean()))\n",
    "print(\"test value minimum: {:.16f}\".format(y_test.min()))\n",
    "print(\"test value maximum: {:.16f}\".format(y_test.max()))\n",
    "a=math.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"mse:\",math.sqrt(mean_squared_error(y_test, preds)))\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "b=mean_absolute_error(y_test, preds)\n",
    "print(\"mae:\",\"{:.16f}\".format(float(mean_absolute_error(y_test, preds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "modellstm = Sequential()\n",
    "modellstm.add(Embedding(num_words, 300, input_length=100, weights= [embedding_matrix], trainable=False))\n",
    "modellstm.add(LSTM(300))\n",
    "modelcnn.add(Dense(32, activation='relu'))\n",
    "modelcnn.add(Dense(16, activation='relu'))\n",
    "modelcnn.add(Dense(4, activation='relu'))\n",
    "\n",
    "combinedInput = concatenate([mlpModel.output, modellstm.output])\n",
    "# our final FC layer head will have two dense layers, the final one\n",
    "# being our regression head\n",
    "x = Dense(4, activation=\"relu\")(combinedInput)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "# our final model will accept categorical/numerical data on the MLP\n",
    "# input and images on the CNN input, outputting a single value (the\n",
    "# predicted price of the house)\n",
    "model = Model(inputs=[mlpModel.input, modellstm.input], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "6086/6086 [==============================] - 1616s 265ms/step - loss: 100437526.5635 - mse: 0.1770 - mae: 0.3990 - mape: 100437526.5635 - val_loss: 8727333.0000 - val_mse: 0.0407 - val_mae: 0.1041 - val_mape: 8727333.0000\n",
      "[INFO] predicting pledged money...\n",
      "33695\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=1e-6, decay=1e-6)\n",
    "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt, metrics=['mse', 'mae', 'mape'])\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "\n",
    "model.fit(\n",
    "\tx=[X_train_nottext, X_train_text], y=y_train,\n",
    "\tvalidation_data=([X_test_nottext, X_test_text], y_test),\n",
    "\tepochs=1)\n",
    "# make predictions on the testing data\n",
    "print(\"[INFO] predicting pledged money...\")\n",
    "preds = model.predict([X_test_nottext, X_test_text])\n",
    "axs=np.unique(preds)\n",
    "print(len(axs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted minimum: 0.0353583097457886\n",
      "predicted maximum: 0.0471842586994171\n",
      "predicted mean: 0.0410560891032219\n",
      "test value mean: 0.1037705110751163\n",
      "test value minimum: 0.0000000000000000\n",
      "test value maximum: 1.0000000000000000\n",
      "mse: 0.2017116543465942\n",
      "mae: 0.1041053031337072\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "print(\"predicted minimum: {:.16f}\".format(preds.min()))\n",
    "print(\"predicted maximum: {:.16f}\".format(preds.max()))\n",
    "print(\"predicted mean: {:.16f}\".format(preds.mean()))\n",
    "print(\"test value mean: {:.16f}\".format(y_test.mean()))\n",
    "print(\"test value minimum: {:.16f}\".format(y_test.min()))\n",
    "print(\"test value maximum: {:.16f}\".format(y_test.max()))\n",
    "a=math.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"mse:\",math.sqrt(mean_squared_error(y_test, preds)))\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "b=mean_absolute_error(y_test, preds)\n",
    "print(\"mae:\",\"{:.16f}\".format(float(mean_absolute_error(y_test, preds))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
