{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Project Delivery #3 - Deep Learning based Text Classification - Predict Success</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    241906\n",
      "1    133956\n",
      "Name: state, dtype: int64\n",
      "name              object\n",
      "main_category       int8\n",
      "country             int8\n",
      "usd_goal_real    float64\n",
      "dtype: object\n",
      "int64\n",
      "                                                name  main_category  country  \\\n",
      "0                    The Songs of Adelaide & Abullah             12        9   \n",
      "1      Greeting From Earth: ZGAC Arts Capsule For ET              6       22   \n",
      "2                                     Where is Hank?              6       22   \n",
      "3  ToshiCapital Rekordz Needs Help to Complete Album             10       22   \n",
      "4  Community Film Project: The Art of Neighborhoo...              6       22   \n",
      "\n",
      "   usd_goal_real  \n",
      "0        1533.95  \n",
      "1       30000.00  \n",
      "2       45000.00  \n",
      "3        5000.00  \n",
      "4       19500.00  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import texthero as hero\n",
    "from texthero import preprocessing\n",
    "\n",
    "ks2018 = pd.read_csv(\"ks-projects-201801.csv\")\n",
    "#ks2016 = pd.read_csv(\"ks-projects-201612.csv\")\n",
    "#print(ks2016.info())\n",
    "\n",
    "df=ks2018\n",
    "\n",
    "# remove currently live campaigns\n",
    "df = df[df.state != 'live']\n",
    "\n",
    "# drop columns we don't need\n",
    "df = df.drop('ID', axis=1)\n",
    "df = df.drop('category', axis=1) #we keep main category\n",
    "df = df.drop('goal', axis=1)\n",
    "df = df.drop('pledged', axis=1)\n",
    "df = df.drop('usd pledged', axis=1)\n",
    "df = df.drop('currency', axis=1)\n",
    "\n",
    "# if successful then 1, else 0, this is our class label\n",
    "df['state'] = df['state'].map(lambda x: 1 if x == \"successful\" else 0)\n",
    "print(df.state.value_counts())\n",
    "\n",
    "\n",
    "df = df[df['name'].notna()]\n",
    "custom_pipeline = [preprocessing.fillna,\n",
    "                   #preprocessing.lowercase,\n",
    "                   preprocessing.remove_whitespace,\n",
    "                   preprocessing.remove_diacritics\n",
    "                   #preprocessing.remove_brackets\n",
    "                  ]\n",
    "df['name'] = hero.clean(df['name'], custom_pipeline)\n",
    "df['name'] = [n.replace('{','') for n in df['name']]\n",
    "df['name'] = [n.replace('}','') for n in df['name']]\n",
    "df['name'] = [n.replace('(','') for n in df['name']]\n",
    "df['name'] = [n.replace(')','') for n in df['name']]\n",
    "\n",
    "X=df\n",
    "X=X.drop(['state', 'deadline', 'launched', 'backers', 'usd_pledged_real'], axis=1)\n",
    "y=df[\"state\"]\n",
    "\n",
    "#encoding category and country //nominal attributes\n",
    "X[\"main_category\"] = X[\"main_category\"].astype('category')\n",
    "X[\"main_category\"] = X[\"main_category\"].cat.codes\n",
    "X[\"country\"] = X[\"country\"].astype('category')\n",
    "X[\"country\"] = X[\"country\"].cat.codes\n",
    "\n",
    "print(X.dtypes)\n",
    "print(y.dtypes)\n",
    "\n",
    "print(X.head())\n",
    "#https://github.com/sdevalapurkar/kickstarter-prediction\n",
    "#https://towardsdatascience.com/how-to-vectorize-text-in-dataframes-for-nlp-tasks-3-simple-techniques-82925a5600db\n",
    "#https://www.kaggle.com/stacykurnikova/using-glove-embedding\n",
    "#https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "#https://djajafer.medium.com/multi-class-text-classification-with-keras-and-lstm-4c5525bef592\n",
    "#https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "#https://realpython.com/python-keras-text-classification/#convolutional-neural-networks-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe data loaded\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.300d.txt',encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0] ## The first entry is the word\n",
    "    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('GloVe data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['songs', 'adelaide', 'abullah'], ['greeting', 'earth', 'zgac', 'arts', 'capsule', 'et'], ['hank'], ['toshicapital', 'rekordz', 'needs', 'help', 'complete', 'album'], ['community', 'film', 'project', 'art', 'neighborhood', 'filmmaking']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "## Iterate over the data to preprocess by removing stopwords\n",
    "lines_without_stopwords=[] \n",
    "for line in X['name'].values: \n",
    "    line = line.lower()\n",
    "    line_by_words = re.findall(r'(?:\\w+)', line, flags = re.UNICODE) # remove punctuation ans split\n",
    "    new_line=[]\n",
    "    for word in line_by_words:\n",
    "        if word not in stop:\n",
    "            new_line.append(word)\n",
    "    lines_without_stopwords.append(new_line)\n",
    "texts = lines_without_stopwords\n",
    "\n",
    "print(texts[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 126532 unique tokens.\n",
      "(300686, 100)\n",
      "(300686, 2)\n"
     ]
    }
   ],
   "source": [
    "## Code adapted from (https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py)\n",
    "# Vectorize the text samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X['name']=texts\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, y, test_size=0.2)\n",
    "\n",
    "MAX_NUM_WORDS = 1000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(y_train))\n",
    "\n",
    "print(data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More code adapted from the keras reference (https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py)\n",
    "# prepare embedding matrix \n",
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "## EMBEDDING_DIM =  ## seems to need to match the embeddings_index dimension\n",
    "EMBEDDING_DIM = embeddings_index.get('a').shape[0]\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word) ## This references the loaded embeddings dictionary\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_24 (Embedding)     (None, 100, 300)          300300    \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 96, 128)           192128    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_7 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 493,740\n",
      "Trainable params: 193,440\n",
      "Non-trainable params: 300,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Code from: https://medium.com/@sabber/classifying-yelp-review-comments-using-cnn-lstm-and-pre-trained-glove-word-embeddings-part-3-53fcea9a17fa\n",
    "## To create and visualize a model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, GlobalMaxPooling1D, Dropout, Activation, MaxPooling1D\n",
    "\n",
    "modelcnn = Sequential()\n",
    "modelcnn.add(Embedding(num_words, 300, input_length=100, weights= [embedding_matrix], trainable=False))\n",
    "modelcnn.add(Conv1D(128, 5, activation='relu'))\n",
    "modelcnn.add(GlobalMaxPooling1D())\n",
    "modelcnn.add(Dense(10, activation='relu'))\n",
    "modelcnn.add(Dense(2, activation='sigmoid'))\n",
    "modelcnn.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "modelcnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8457/8457 [==============================] - 187s 22ms/step - loss: 0.6034 - accuracy: 0.6629 - val_loss: 0.5895 - val_accuracy: 0.6754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1963745df28>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fit train data\n",
    "modelcnn.fit(data, np.array(labels), validation_split=0.1, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 146149 unique tokens.\n",
      "(75172, 100)\n",
      "(75172, 2)\n",
      "2350/2350 [==============================] - 18s 8ms/step - loss: 0.6322 - accuracy: 0.6373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6322205662727356, 0.6372851729393005]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(X_test)\n",
    "sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "test = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels_test = to_categorical(np.asarray(y_test))\n",
    "print(test.shape)\n",
    "print(labels_test.shape)\n",
    "\n",
    "modelcnn.evaluate(test, np.array(labels_test)) #cnn test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (None, 100, 300)          300300    \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 1,022,102\n",
      "Trainable params: 721,802\n",
      "Non-trainable params: 300,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modellstm = Sequential()\n",
    "modellstm.add(Embedding(num_words, 300, input_length=100, weights= [embedding_matrix], trainable=False))\n",
    "modellstm.add(LSTM(300))\n",
    "modellstm.add(Dense(2, activation='sigmoid'))\n",
    "modellstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "modellstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8457/8457 [==============================] - 2443s 289ms/step - loss: 0.5991 - accuracy: 0.6679 - val_loss: 0.5893 - val_accuracy: 0.6744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x196458b1d30>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Fit train data\n",
    "modellstm.fit(data, np.array(labels), validation_split=0.1, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2350/2350 [==============================] - 252s 107ms/step - loss: 0.6280 - accuracy: 0.6333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6280394196510315, 0.6333076357841492]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modellstm.evaluate(test, np.array(labels_test)) #lstm test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
