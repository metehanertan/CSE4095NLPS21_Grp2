{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Project Delivery #3 - Deep Learning based Text Classification - Predict Success</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    241906\n",
      "1    133956\n",
      "Name: state, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import texthero as hero\n",
    "from texthero import preprocessing\n",
    "\n",
    "ks2018 = pd.read_csv(\"ks-projects-201801.csv\")\n",
    "#ks2016 = pd.read_csv(\"ks-projects-201612.csv\")\n",
    "#print(ks2016.info())\n",
    "\n",
    "df=ks2018\n",
    "\n",
    "# remove currently live campaigns\n",
    "df = df[df.state != 'live']\n",
    "\n",
    "# drop columns we don't need\n",
    "df = df.drop('ID', axis=1)\n",
    "df = df.drop('category', axis=1) #we keep main category\n",
    "#df = df.drop('goal', axis=1)\n",
    "df = df.drop('pledged', axis=1)\n",
    "df = df.drop('usd pledged', axis=1)\n",
    "df = df.drop('currency', axis=1)\n",
    "\n",
    "# if successful then 1, else 0, this is our class label\n",
    "df['state'] = df['state'].map(lambda x: 1 if x == \"successful\" else 0)\n",
    "print(df.state.value_counts())\n",
    "\n",
    "\n",
    "df = df[df['name'].notna()]\n",
    "custom_pipeline = [preprocessing.fillna,\n",
    "                   #preprocessing.lowercase,\n",
    "                   preprocessing.remove_whitespace,\n",
    "                   preprocessing.remove_diacritics\n",
    "                   #preprocessing.remove_brackets\n",
    "                  ]\n",
    "df['name'] = hero.clean(df['name'], custom_pipeline)\n",
    "df['name'] = [n.replace('{','') for n in df['name']]\n",
    "df['name'] = [n.replace('}','') for n in df['name']]\n",
    "df['name'] = [n.replace('(','') for n in df['name']]\n",
    "df['name'] = [n.replace(')','') for n in df['name']]\n",
    "\n",
    "X=df\n",
    "X=X.drop(['state', 'deadline', 'launched', 'backers', 'usd_pledged_real'], axis=1)\n",
    "y=df[\"state\"]\n",
    "\n",
    "#encoding category and country //nominal attributes\n",
    "X[\"main_category\"] = X[\"main_category\"].astype('category')\n",
    "X[\"main_category\"] = X[\"main_category\"].cat.codes\n",
    "X[\"country\"] = X[\"country\"].astype('category')\n",
    "X[\"country\"] = X[\"country\"].cat.codes\n",
    "\n",
    "#https://github.com/sdevalapurkar/kickstarter-prediction\n",
    "#https://towardsdatascience.com/how-to-vectorize-text-in-dataframes-for-nlp-tasks-3-simple-techniques-82925a5600db\n",
    "#https://www.kaggle.com/stacykurnikova/using-glove-embedding\n",
    "#https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "#https://djajafer.medium.com/multi-class-text-classification-with-keras-and-lstm-4c5525bef592\n",
    "#https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "#https://realpython.com/python-keras-text-classification/#convolutional-neural-networks-cnn\n",
    "#https://www.pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe data loaded\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.300d.txt',encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0] ## The first entry is the word\n",
    "    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('GloVe data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "## Iterate over the data to preprocess by removing stopwords\n",
    "lines_without_stopwords=[] \n",
    "for line in X['name'].values: \n",
    "    line = line.lower()\n",
    "    line_by_words = re.findall(r'(?:\\w+)', line, flags = re.UNICODE) # remove punctuation ans split\n",
    "    new_line=[]\n",
    "    for word in line_by_words:\n",
    "        if word not in stop:\n",
    "            new_line.append(word)\n",
    "    lines_without_stopwords.append(new_line)\n",
    "texts = lines_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 126185 unique tokens.\n",
      "Found 51163 unique tokens.\n",
      "(300686, 100)\n",
      "(75172, 100)\n"
     ]
    }
   ],
   "source": [
    "## Code adapted from (https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py)\n",
    "# Vectorize the text samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X['name']=texts\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,shuffle=True)\n",
    "\n",
    "MAX_NUM_WORDS = 1000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "tokenizertrain = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizertrain.fit_on_texts(X_train[\"name\"])\n",
    "sequences = tokenizertrain.texts_to_sequences(X_train[\"name\"])\n",
    "\n",
    "word_index = tokenizertrain.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X_train_text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "tokenizertest = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizertest.fit_on_texts(X_test[\"name\"])\n",
    "sequences = tokenizertest.texts_to_sequences(X_test[\"name\"])\n",
    "\n",
    "word_index = tokenizertest.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "X_test_text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "X_train_nottext=X_train.drop('name', axis=1)\n",
    "X_test_nottext=X_test.drop('name', axis=1)\n",
    "\n",
    "y_train = to_categorical(np.asarray(y_train))\n",
    "y_test = to_categorical(np.asarray(y_test))\n",
    "\n",
    "print(X_train_text.shape)\n",
    "print(X_test_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More code adapted from the keras reference (https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py)\n",
    "# prepare embedding matrix \n",
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, GlobalMaxPooling1D, Dropout, Activation, MaxPooling1D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "## EMBEDDING_DIM =  ## seems to need to match the embeddings_index dimension\n",
    "EMBEDDING_DIM = embeddings_index.get('a').shape[0]\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word) ## This references the loaded embeddings dictionary\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim, regress=False): #Multi-layer perceptron for nominal, numeric data\n",
    "\t# define our MLP network\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "\tmodel.add(Dense(4, activation=\"relu\"))\n",
    "\tmodel.add(Dense(2, activation=\"sigmoid\"))\n",
    "\t# check to see if the regression node should be added\n",
    "\tif regress:\n",
    "\t\tmodel.add(Dense(1, activation=\"linear\"))\n",
    "\t# return our model\n",
    "\treturn model\n",
    "\n",
    "mlpModel=create_mlp(X_train_nottext.shape[1], regress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 300)          300300    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 96, 128)           192128    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 493,740\n",
      "Trainable params: 193,440\n",
      "Non-trainable params: 300,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Code from: https://medium.com/@sabber/classifying-yelp-review-comments-using-cnn-lstm-and-pre-trained-glove-word-embeddings-part-3-53fcea9a17fa\n",
    "## To create and visualize a model\n",
    "\n",
    "modelcnn = Sequential()\n",
    "modelcnn.add(Embedding(num_words, 300, input_length=100, weights= [embedding_matrix], trainable=False))\n",
    "modelcnn.add(Conv1D(128, 5, activation='relu'))\n",
    "modelcnn.add(GlobalMaxPooling1D())\n",
    "modelcnn.add(Dense(10, activation='relu'))\n",
    "modelcnn.add(Dense(2, activation='sigmoid'))\n",
    "modelcnn.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "modelcnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedInput = concatenate([mlpModel.output, modelcnn.output])\n",
    "# our final FC layer head will have two dense layers, the final one\n",
    "# being our regression head\n",
    "x = Dense(4, activation=\"relu\")(combinedInput)\n",
    "x = Dense(2, activation=\"sigmoid\")(x)\n",
    "# our final model will accept categorical/numerical data on the MLP\n",
    "# input and texts on the CNN input, outputting a single value //successful or not\n",
    "model = Model(inputs=[mlpModel.input, modelcnn.input], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "8457/8457 [==============================] - 161s 19ms/step - loss: 0.6546 - accuracy: 0.6433 - val_loss: 0.6247 - val_accuracy: 0.6430\n",
      "[INFO] predicting success...\n",
      "2350/2350 [==============================] - 17s 7ms/step - loss: 0.6400 - accuracy: 0.6451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6399879455566406, 0.6450939178466797]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = Adam(lr=1e-3, decay=1e-3)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "\n",
    "model.fit(\n",
    "\tx=[X_train_nottext, X_train_text], y=y_train,\n",
    "\tvalidation_split=0.1,\n",
    "\tepochs=1)\n",
    "# make predictions on the testing data\n",
    "print(\"[INFO] predicting success...\")\n",
    "model.evaluate([X_test_nottext, X_test_text], y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 300)          300300    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 1,022,102\n",
      "Trainable params: 721,802\n",
      "Non-trainable params: 300,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modellstm = Sequential()\n",
    "modellstm.add(Embedding(num_words, 300, input_length=100, weights= [embedding_matrix], trainable=False))\n",
    "modellstm.add(LSTM(300))\n",
    "modellstm.add(Dense(2, activation='sigmoid'))\n",
    "modellstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "modellstm.summary()\n",
    "\n",
    "combinedInput = concatenate([mlpModel.output, modellstm.output])\n",
    "# our final FC layer head will have two dense layers, the final one\n",
    "# being our regression head\n",
    "x = Dense(4, activation=\"relu\")(combinedInput)\n",
    "x = Dense(2, activation=\"sigmoid\")(x)\n",
    "# our final model will accept categorical/numerical data on the MLP\n",
    "# input and images on the CNN input, outputting a single value (the\n",
    "# predicted price of the house)\n",
    "model = Model(inputs=[mlpModel.input, modellstm.input], outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training model...\n",
      "8457/8457 [==============================] - 2201s 260ms/step - loss: 0.6044 - accuracy: 0.6625 - val_loss: 0.5771 - val_accuracy: 0.6803\n",
      "[INFO] predicting success...\n",
      "2350/2350 [==============================] - 244s 104ms/step - loss: 0.6042 - accuracy: 0.6537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6041766405105591, 0.6537407636642456]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = Adam(lr=1e-3, decay=1e-3)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "\n",
    "model.fit(\n",
    "\tx=[X_train_nottext, X_train_text], y=y_train,\n",
    "\tvalidation_split=0.1,\n",
    "\tepochs=1)\n",
    "# make predictions on the testing data\n",
    "print(\"[INFO] predicting success...\")\n",
    "model.evaluate([X_test_nottext, X_test_text], y=y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
